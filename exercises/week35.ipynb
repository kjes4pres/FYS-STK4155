{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc5bbcf5",
   "metadata": {},
   "source": [
    "# Week 35\n",
    "### Kjersti Stangeland, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e88c69",
   "metadata": {},
   "source": [
    "## Exercise 1 - Finding the derivative of Matrix-Vector expressions\n",
    "\n",
    "**a)** Consider the expression\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{x})}{\\partial \\boldsymbol{x}},\n",
    "$$\n",
    "\n",
    "Where $\\boldsymbol{a}$ and $\\boldsymbol{x}$ are column-vectors with length $n$.\n",
    "\n",
    "What is the *shape* of the expression we are taking the derivative of?\n",
    "\n",
    "What is the *shape* of the thing we are taking the derivative with respect to?\n",
    "\n",
    "What is the *shape* of the result of the expression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc42263",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**b)** Show that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{x})}{\\partial \\boldsymbol{x}} = \\boldsymbol{a}^T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d1662",
   "metadata": {},
   "source": [
    "**Answer a & b**:\n",
    "\n",
    "\n",
    "We have $\\bold{a} = \\begin{pmatrix} a_0 \\\\ a_1 \\\\ ... \\\\ a_{n-1} \\end{pmatrix} (n, 1)$ , $\\bold{a^T} = \\begin{pmatrix} a_0 & a_1 & ... & a_{n-1} \\end{pmatrix} (1, n)$ and $\\bold{x} = \\begin{pmatrix} x_0 \\\\ x_1 \\\\ ... \\\\ x_{n-1} \\end{pmatrix} (n, 1)$. Thus, the expression we're taking the derivative of is $\\boldsymbol{a}^T\\boldsymbol{x} = \\begin{pmatrix} a_0 & a_1 & ... & a_{n-1} \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\\\ ... \\\\ x_{n-1} \\end{pmatrix} = \\begin{pmatrix} a_0 x_0 + a_1 x_1+ ... + a_{n-1} x_{n-1} \\end{pmatrix}$ or more neatly $\\boldsymbol{a}^T\\boldsymbol{x} = \\sum_{i=0}^{n-1}a_i x_i$ and is a scalar with shape $(1, 1)$. \n",
    "\n",
    "The *thing* we are taking the derivative with respect to, $\\bold{x} = \\begin{pmatrix} x_0 \\\\ x_1 \\\\ ... \\\\ x_{n-1} \\end{pmatrix}$ has shape $(n, 1)$.\n",
    "\n",
    "The expression is taking the derivative of a scalar $(1,1)$ with respect to a vector $(n, 1)$. That is, how does the scalar change when each component of $x$ changes. Writing it out:\n",
    "\n",
    "\n",
    "$\\frac{\\partial(\\boldsymbol{a}^T\\boldsymbol{x})}{\\partial \\bold{x}} = \\frac{\\partial}{\\partial x_j}(\\sum_{i=0}^{n}a_i x_i)$ where $ \\frac{\\partial}{\\partial x_j}$ is the partial derivative and $j=0, 1, 2, ..., n-1$. Further writing it out, $\\frac{\\partial}{\\partial x_j}(\\sum_{i=0}^{n-1}a_i x_i) = \\sum_{i=0}^{n-1}a_i \\frac{\\partial x_i}{\\partial x_j}$ where we have if $i=j, \\frac{\\partial x_i}{\\partial x_j} = 1$ or $i\\neq j, \\frac{\\partial x_i}{\\partial x_j} = 0$. The result is then $\\frac{\\partial(\\boldsymbol{a}^T\\boldsymbol{x})}{\\partial \\bold{x}} = \\frac{\\partial}{\\partial x_j}(\\sum_{i=0}^{n}a_i x_i) = \\sum_{i=0}^{n}a_i \\frac{\\partial x_i}{\\partial x_j} = a_j$ and $\\frac{\\partial(\\boldsymbol{a}^T\\boldsymbol{x})}{\\partial \\bold{x}} = \\bold{a}^T$. \n",
    "\n",
    "Here I would argue that as you can write both $=a_j$ and $a_i$ that the shape of the resulting expression depends on which method you use. Therefore both $(1, n)$ and $(n, 1)$ are valid shapes of the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72544835",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**c)** Show that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{A}\\boldsymbol{a})}{\\partial \\boldsymbol{a}} = \\boldsymbol{a}^T(\\boldsymbol{A}+\\boldsymbol{A}^T)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f447e8",
   "metadata": {},
   "source": [
    "__Answer c__ :\n",
    "\n",
    "I assume that:\n",
    "\n",
    "* $\\bold{a^T}$ has shape $(1, n)$\n",
    "* $\\bold{A}$ has shape $(n, n)$\n",
    "* $\\bold{a}$ has shape $(n, 1)$\n",
    "\n",
    "Starting with $$\\bold{a^T}\\bold{A}\\bold{a} = \\begin{pmatrix} a_0 & a_1 & ... & a_n-1 \\end{pmatrix} \\begin{bmatrix} a_{00} & a_{01} & \\cdots & a_{0n-1} \\\\ a_{10} & a_{11} & \\cdots & a_{1n-1} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n-1 0} & a_{n-1 1} & \\cdots & a_{n-1 n-1} \\end{bmatrix}\\begin{pmatrix} a_0 \\\\ a_1 \\\\ ... \\\\ a_n-1 \\end{pmatrix}$$\n",
    "\n",
    "\n",
    "We can define the scalar $\\alpha = \\bold{a^T}\\bold{A}\\bold{a}$ which written out, component-wise looks like: $\\alpha = \\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} a_i A_{ij} a_j$. Taking the derivative of $\\alpha$ with respect to each component of $\\bold a$, \n",
    "$$\\frac{\\partial \\alpha}{\\partial {a_k}} = \\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} \\frac{\\partial}{\\partial {a_k}} (a_i A_{ij} a_j)$$\n",
    "\n",
    "If $i=k$ or $j=k$ the derivative is non-zero. We can write this using the Kronecker delta: \n",
    "$$\\frac{\\partial \\alpha}{\\partial {a_k}} = \\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} (\\delta_{ik}A_{ij}a_j + \\delta_{jk}A_{ij}a_i)$$\n",
    "such that if $i=k$ or $j=k$, $\\delta = 1$. And as $\\bold a$ is a 1-dimensional vector, the components of the transposed equal the untransposed, $a_i = a^T_j$. This leaves us with: \n",
    "\n",
    "$$\\frac{\\partial \\alpha}{\\partial {a_k}} = \\sum_{i=0}^{n-1} A_{ik} a_i + \\sum_{j=0}^{n-1} A_{kj} a_j = \\sum_{j=0}^{n-1} (A_{kj} + A_{jk}) a_j$$ \n",
    "\n",
    "and lastly back to matrix form: \n",
    "\n",
    "$$\\frac{\\partial \\alpha}{\\partial \\bold a} = \\bold a^T(\\boldsymbol{A}+\\boldsymbol{A}^T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e2c1af",
   "metadata": {},
   "source": [
    "## Exercise 2 - Deriving the expression for OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e52fe",
   "metadata": {},
   "source": [
    "The ordinary least squares method finds the parameters $\\boldsymbol{\\theta}$ which minimizes the squared error between our model $\\boldsymbol{X\\theta}$ and the true values $\\boldsymbol{y}$.\n",
    "\n",
    "To find the parameters $\\boldsymbol{\\theta}$ which minimizes this error, we take the derivative of the squared error expression with respect to $\\boldsymbol{\\theta}$, and set it equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945a1c0c",
   "metadata": {},
   "source": [
    "**a)** Very briefly explain why the approach above finds the parameters $\\boldsymbol{\\theta}$ which minimizes this error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9766508",
   "metadata": {},
   "source": [
    "__Answer a__:\n",
    "\n",
    "The approach find the parameters $\\boldsymbol{\\theta}$ which minimizes error because we find the minimum of the error function with respect to the parameters. The parameters are in our function to reproduce the true values. As the error function compares the true values with modelled values, which depend on the parameters, minimizing the function gives us the optimal parameters. This works as the squared error is convex, and thus finding the minimum yields the lowest error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c8e1f",
   "metadata": {},
   "source": [
    "We typically write the squared error as\n",
    "\n",
    "$$\n",
    "\\vert\\vert\\boldsymbol{y} - \\boldsymbol{X\\theta}\\vert\\vert^2\n",
    "$$\n",
    "\n",
    "which we can rewrite in matrix-vector form as\n",
    "\n",
    "$$\n",
    "\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc018be8",
   "metadata": {},
   "source": [
    "**b)** If $\\boldsymbol{X}$ is invertible, what is the expression for the optimal parameters $\\boldsymbol{\\theta}$? (**Hint:** Don't compute any derivatives, but solve $\\boldsymbol{X\\theta}=\\boldsymbol{y}$ for $\\boldsymbol{\\theta}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55adc0e",
   "metadata": {},
   "source": [
    "__Answer b:__\n",
    "\n",
    "If $\\boldsymbol{X}$ is invertible we can directly solve $\\boldsymbol{X\\theta}=\\boldsymbol{y}$ as $\\boldsymbol{\\theta} = \\boldsymbol{X^{-1}}\\boldsymbol{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee520e",
   "metadata": {},
   "source": [
    "**c)** Show that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)}{\\partial \\boldsymbol{s}} = -2\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\boldsymbol{A},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14afc01",
   "metadata": {},
   "source": [
    "__Answer c:__\n",
    "First lets expand the numerator:\n",
    "\n",
    "$$\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right) =\n",
    "\n",
    "\\bold x^T \\bold x - \\bold x^T \\boldsymbol{As} - \\boldsymbol{s^TA^T} \\bold x +\\boldsymbol{s^TA^T}\\boldsymbol{As}$$\n",
    "\n",
    "Since $\\bold x^T \\boldsymbol{As}$ is a scalar it is equal to its transposed. That is: $\\bold x^T \\boldsymbol{As} = (\\bold x^T \\boldsymbol{As})^T = \\bold s^T \\bold A^T \\bold x$. This leaves us \n",
    "with \n",
    "\n",
    "$$\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right) =\n",
    "\n",
    "\\bold x^T \\bold x - 2\\boldsymbol{s^TA^T} \\bold x +\\boldsymbol{s^TA^T}\\boldsymbol{As}\n",
    "\n",
    "$$\n",
    "\n",
    "Now we can derivate each term with respect to $\\bold s$. The first term drops out as it is independent of $\\bold s$.\n",
    "\n",
    "$$\n",
    "-2\\frac{\\partial}{\\partial \\bold s} (\\boldsymbol{s^TA^T} \\bold x) + \\frac{\\partial}{\\partial \\bold s} (\\boldsymbol{s^TA^T}\\boldsymbol{As})\n",
    "\n",
    "=\n",
    "\n",
    "-2(\\boldsymbol{A^T}\\bold x)^T + 2\\bold s^T \\bold A^T \\bold A\n",
    "\n",
    "=\n",
    "\n",
    "-2(\\bold x - \\bold A \\bold s )^T \\bold A\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df895c6c",
   "metadata": {},
   "source": [
    "**d)** Using the expression from **c)**, but substituting back in $\\boldsymbol{\\theta}$, $\\boldsymbol{y}$ and $\\boldsymbol{X}$, find the expression for the optimal parameters $\\boldsymbol{\\theta}$ in the case that $\\boldsymbol{X}$ is not invertible, but $\\boldsymbol{X^T X}$ is, which is most often the case.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{\\theta}_{OLS}} = ...\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ba58b",
   "metadata": {},
   "source": [
    "__Answer d:__\n",
    "Inserting into the expression above:\n",
    "\n",
    "$$\\boldsymbol{\\hat{\\theta}_{OLS}} =\n",
    "\\frac{\\partial (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})^T (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = \n",
    "-2(\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})^T \\boldsymbol{X} = \n",
    "0\n",
    "$$\n",
    "\n",
    "Then we can solve for $\\boldsymbol{\\hat{\\theta}}$:\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\hat{\\theta}})^T \\boldsymbol{X} = 0\n",
    "$$\n",
    "\n",
    "As $\\boldsymbol{X}$ is not invertible, but $\\boldsymbol{X^T X}$ is, we can transpose the equation, solve the paranthesis and invert $\\boldsymbol{X^T X}$ to find a solution.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X^T}(\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\hat{\\theta}})  = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X^T}\\boldsymbol{y} - \\boldsymbol{X^T X}\\ \\boldsymbol{\\hat{\\theta}}  = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X^T X}\\ \\boldsymbol{\\hat{\\theta}}  = \\boldsymbol{X^T}\\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{\\theta}}  = (\\boldsymbol{X^T X})^{-1}\\boldsymbol{X^T}\\boldsymbol{y}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0328eb",
   "metadata": {},
   "source": [
    "## Exercise 3 - Creating feature matrix and implementing OLS using the analytical expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1d15d",
   "metadata": {},
   "source": [
    "With the expression for $\\boldsymbol{\\hat{\\theta}_{OLS}}$, you now have what you need to implement OLS regression with your input data and target data $\\boldsymbol{y}$. But before you can do that, you need to set up you input data as a feature matrix $\\boldsymbol{X}$.\n",
    "\n",
    "In a feature matrix, each row is a datapoint and each column is a feature of that data. If you want to predict someones spending based on their income and number of children, for instance, you would create a row for each person in your dataset, with the montly income and the number of children as columns.\n",
    "\n",
    "We typically also include an intercept in our models. The intercept is a value that is added to our prediction regardless of the value of the other features. The intercept tries to account for constant effects in our data that are not dependant on anything else. In our current example, the intercept could account for living expenses which are typical regardless of income or childcare expenses.\n",
    "\n",
    "We calculate the optimal intercept by including a feature with the constant value of 1 in our model, which is then multplied by some parameter $\\theta_0$ from the OLS method into the optimal intercept value (which will be $\\theta_0$). In practice, we include the intercept in our model by adding a column of ones to the start of our feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f89541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "income = np.array([116., 161., 167., 118., 172., 163., 179., 173., 162., 116., 101., 176., 178., 172., 143., 135., 160., 101., 149., 125.])\n",
    "children = np.array([5, 3, 0, 4, 5, 3, 0, 4, 4, 3, 3, 5, 1, 0, 2, 3, 2, 1, 5, 4])\n",
    "spending = np.array([152., 141., 102., 136., 161., 129.,  99., 159., 160., 107.,  98., 164., 121.,  93., 112., 127., 117.,  69., 156., 131.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfd7ad2",
   "metadata": {},
   "source": [
    "**a)** Create a feature matrix $\\boldsymbol{X}$ for the features income and children, including an intercept column of ones at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ad2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((n, 3))\n",
    "#X[:, 0] = ...\n",
    "#X[:, 1] = ...\n",
    "#X[:, 2] = ...y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c119e66a",
   "metadata": {},
   "source": [
    "**b)** Use the expression from **3d)** to find the optimal parameters $\\boldsymbol{\\hat{\\beta}_{OLS}}$ for predicting spending based on these features. Create a function for this operation, as you are going to need to use it a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS_parameters(X, y):\n",
    "    return ...\n",
    "\n",
    "#beta = OLS_parameters(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb9acb",
   "metadata": {},
   "source": [
    "## Exercise 4 - Fitting a polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df0c859",
   "metadata": {},
   "source": [
    "In this course, we typically do linear regression using polynomials, though in real world applications it is also very common to make linear models based on measured features like you did in the previous exercise.\n",
    "\n",
    "When fitting a polynomial with linear regression, we make each polynomial degree($x, x^2, x^3, ..., x^p$) its own feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.linspace(-3, 3, n)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + np.random.normal(0, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c186345",
   "metadata": {},
   "source": [
    "**a)** Create a feature matrix $\\boldsymbol{X}$ for the features $x, x^2, x^3, x^4, x^5$, including an intercept column of ones at the start. Make this into a function, as you will do this a lot over the next weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e140b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, p):\n",
    "    n = len(x)\n",
    "    X = np.zeros((n, p + 1))\n",
    "    #X[:, 0] = ...\n",
    "    #X[:, 1] = ...\n",
    "    #X[:, 2] = ...\n",
    "    # could this be a loop?\n",
    "\n",
    "#X = polynomial_features(x, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392377fa",
   "metadata": {},
   "source": [
    "**b)** Use the expression from **3d)** to find the optimal parameters $\\boldsymbol{\\hat{\\beta}_{OLS}}$ for predicting $\\boldsymbol{y}$ based on these features. If you have done everything right so far, this code will not need changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530a2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta = OLS_parameters(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85752ddf",
   "metadata": {},
   "source": [
    "**c)** Like in exercise 4 last week, split your feature matrix and target data into a training split and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df051c",
   "metadata": {},
   "source": [
    "**d)** Train your model on the training data(find the parameters which best fit) and compute the MSE on both the training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d0aba",
   "metadata": {},
   "source": [
    "**e)** Do the same for each polynomial degree from 2 to 10, and plot the MSE on both the training and test data as a function of polynomial degree. The aim is to reproduce Figure 2.11 of [Hastie et al](https://github.com/CompPhysics/MLErasmus/blob/master/doc/Textbooks/elementsstat.pdf). Feel free to read the discussions leading to figure 2.11 of Hastie et al. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7c85a",
   "metadata": {},
   "source": [
    "**f)** Interpret the graph. Why do the lines move as they do? What does it tell us about model performance and generalizability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c81f2",
   "metadata": {},
   "source": [
    "## Exercise 5 - Comparing your code with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a697c55",
   "metadata": {},
   "source": [
    "When implementing different algorithms for the first time, it can be helpful to double check your results with established implementations before you go on to add more complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c246f",
   "metadata": {},
   "source": [
    "**a)** Make sure your `polynomial_features` function creates the same feature matrix as sklearns PolynomialFeatures.\n",
    "\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b88db8",
   "metadata": {},
   "source": [
    "**b)** Make sure your `OLS_parameters` function computes the same parameters as sklearns LinearRegression with fit_intercept set to False, since the intercept is included in the feature matrix. Use `your_model_object.coef_` to extract the computed parameters.\n",
    "\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
